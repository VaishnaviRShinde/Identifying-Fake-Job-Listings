{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21657,"status":"ok","timestamp":1724939098015,"user":{"displayName":"Vaishnavi Raman Shinde","userId":"03069921352989706694"},"user_tz":-330},"id":"DocNS98373c9","outputId":"b8f3f05f-fdf1-447e-f874-cb479c2ed884"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["pip install pandas numpy google-generativeai"],"metadata":{"id":"NGINM-leix9G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725024092889,"user_tz":-330,"elapsed":7565,"user":{"displayName":"Vaishnavi Raman Shinde","userId":"03069921352989706694"}},"outputId":"66b8c5e5-770d-4f79-a51b-d184e4fa96ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.7.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.6)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.137.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.8.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai) (1.24.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.20.1)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.64.1)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.48.2)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.4)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.7.4)\n"]}]},{"cell_type":"code","source":["'''This Python script uses pandas to handle a dataset and 'google-generativeai' to fill in missing values using an AI model.\n","It configures the API key for the AI model and defines functions to generate content with the \"gemini-pro\" model and to clean the\n","dataset by filling in missing data. The script loads a CSV file, uses the AI model to suggest and fill missing values based on the\n","dataset's context, and saves the cleaned dataset as \"cleaned_dataset.csv\". The script also handles errors and retries if rate limits are exceeded.'''"],"metadata":{"id":"GbhU_2J_7UZd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import google.generativeai as genai\n","import time\n","\n","# Configure the API key for the generative AI model\n","genai.configure(api_key=\"AIzaSyDklFWkA1SST6f-FxTx6Cf1UR0BJ3HYFjY\")\n","\n","# Function to generate content using the generative AI model\n","def generate_gemini_content(prompt):\n","    while True:\n","        try:\n","            model = genai.GenerativeModel(\"gemini-pro\")\n","            response = model.generate_content(prompt)\n","            return response.text\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")\n","            print(\"Rate limit exceeded or another issue occurred, retrying after delay...\")\n","            time.sleep(60)  # Sleep for 60 seconds before retrying\n","\n","# Function to clean and fill missing values in a dataset\n","def clean_and_fill_missing_values(df):\n","    for column in df.columns:\n","        # Identify rows with missing values\n","        missing_indices = df[df[column].isnull()].index\n","\n","        for index in missing_indices:\n","            # Create a prompt for the generative model based on the column and other values in the row\n","            prompt = f\"Fill in the missing value for column '{column}' based on the following context:\\n\"\n","            context = df.dropna().to_dict(orient='records')\n","            context_string = '\\n'.join([str(record) for record in context])\n","            prompt += f\"Context:\\n{context_string}\\nMissing value location: {df.loc[index].to_dict()}\\n\"\n","            prompt += f\"Suggested value for column '{column}':\"\n","\n","            # Get the suggested value from the generative AI model\n","            suggested_value = generate_gemini_content(prompt).strip()\n","\n","            # Fill the missing value\n","            df.at[index, column] = suggested_value\n","\n","    return df\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Load your dataset\n","    file_path = \"/content/drive/MyDrive/Hackathon/job_train.csv\"  # Replace with your file path\n","    df = pd.read_csv(file_path)\n","\n","    # Clean and fill missing values\n","    cleaned_df = clean_and_fill_missing_values(df)\n","\n","    # Save the cleaned dataset\n","    cleaned_df.to_csv(\"cleaned_dataset.csv\", index=False)\n","    print(\"Missing values have been filled and dataset saved as 'cleaned_dataset.csv'.\")\n"],"metadata":{"id":"yWf6n_twiyRz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''This code snippet reads a CSV file named 'cleaned_dataset.csv' from a specified Google Drive path into a Pandas DataFrame ('df').\n","It then calculates the number of rows in the dataset using the 'len()' function and stores this count in the variable 'num_rows'.\n","Finally, it prints out the total number of rows in the dataset. This is useful for quickly assessing the size of the dataset you are working with.'''"],"metadata":{"id":"lkqZ27aYiycc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2786,"status":"ok","timestamp":1724737066209,"user":{"displayName":"Vaishnavi Raman Shinde","userId":"03069921352989706694"},"user_tz":-330},"id":"OiW-SQpgS6sk","outputId":"e407cb82-8639-4f78-9d9a-82be08740a01"},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of rows in the dataset: 8940\n"]}],"source":["import pandas as pd\n","df = pd.read_csv('/content/drive/MyDrive/Dataset/cleaned_dataset.csv')\n","num_rows = len(df)\n","\n","print(f\"The number of rows in the dataset: {num_rows}\")"]},{"cell_type":"code","source":["'''This Python script is designed to clean a dataset by removing rows that contain specific unwanted values, such as 'UNKNOWN' or 'NAME',\n","in any column. The process begins by reading the dataset from a CSV file ('cleaned_dataset.csv') stored in Google Drive into a Pandas\n","DataFrame ('df'). It then filters out rows where any column contains the values 'UNKNOWN' or 'NAME', resulting in a cleaned DataFrame ('df_cleaned').\n","The number of rows after this cleaning operation is calculated and printed, giving an indication of how much data was removed. Finally, the\n","cleaned DataFrame is saved as a new CSV file named 'cleaned_dataset1.csv', and a confirmation message is printed to indicate that the cleaning\n","process is complete and the cleaned dataset has been saved.'''"],"metadata":{"id":"NgtLqriw8mBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1260,"status":"ok","timestamp":1724737566277,"user":{"displayName":"Vaishnavi Raman Shinde","userId":"03069921352989706694"},"user_tz":-330},"id":"HPaduLn9V1-E","outputId":"dfd6391a-b4e8-430d-c12f-92cc1a788f51"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                  title             location  \\\n","0                        Architect Middleware MQ Kuwait                KW KU   \n","2     Process Controls Staff Engineer Foxboro I A Tr...  US TX USA Southwest   \n","3     Experienced Telemarketer Wanted Digital Solutions               AU NSW   \n","4                               Senior Network Engineer        GB ENG London   \n","5     Energy Financial Reporter Low Carbon Energy In...       US NY New York   \n","...                                                 ...                  ...   \n","8934                          Data Manager Fixed Income       US NY New York   \n","8935                                  Financial Analyst         GR I Paiania   \n","8936               Customer Service Associate Part Time   CA ON Peterborough   \n","8937                                      Sales Manager       UA 61 Ternopil   \n","8939                               Sale Representatives                US NY   \n","\n","                                            description  \\\n","0     On behalf of our client a well known multinati...   \n","2     Experienced Process Controls Staff Engineer is...   \n","3     If you have a passion for people and love to s...   \n","4     As the successful Senior Network Engineer you ...   \n","5     Energy financial reporter needed in NYCDriven ...   \n","...                                                 ...   \n","8934  The salary is open DOE This role manages 4 Dat...   \n","8935  Financial analysis reporting and review of dep...   \n","8936  The Customer Service Associate will be based i...   \n","8937  Responsibilitiesactive sales realization of co...   \n","8939  Sales Representative Job Purpose To serve cust...   \n","\n","                                           requirements  telecommuting  \\\n","0     Working technical knowledge of IT systems and ...              0   \n","2     At least 10 years of degreed professional expe...              0   \n","3     Responsibilities Prospecting following up and ...              0   \n","4     Essential skills Juniper switching routing sec...              0   \n","5     The successful candidate should have a bachelo...              0   \n","...                                                 ...            ...   \n","8934  Required Skills BA BS in Business Administrati...              0   \n","8935  Postgraduate degree required Economics or Fina...              0   \n","8936  Minimum Requirements Minimum of 6 months custo...              0   \n","8937  Main requirementsexperience with Sales 2 years...              0   \n","8939  Our website is under construction Take a momen...              0   \n","\n","      has_company_logo  has_questions  fraudulent  \n","0                    1              0           0  \n","2                    0              0           0  \n","3                    1              0           0  \n","4                    1              0           0  \n","5                    1              1           0  \n","...                ...            ...         ...  \n","8934                 0              0           0  \n","8935                 1              1           0  \n","8936                 1              0           0  \n","8937                 0              1           0  \n","8939                 1              0           0  \n","\n","[7507 rows x 8 columns]\n","The number of rows after cleaning: 7507\n","Cleaned dataset saved as 'cleaned_dataset1.csv'\n"]}],"source":["import pandas as pd\n","\n","# Load your dataset\n","# Replace 'your_dataset.csv' with the actual path to your CSV file\n","df = pd.read_csv('/content/drive/MyDrive/Dataset/cleaned_dataset.csv')\n","\n","# Removing rows with 'UNKNOWN' or 'NAME' in any column\n","df_cleaned = df[~df.isin(['UNKNOWN', 'NAME']).any(axis=1)]\n","\n","# Print the cleaned DataFrame\n","print(df_cleaned)\n","num_rows_after = len(df_cleaned)\n","print(f\"The number of rows after cleaning: {num_rows_after}\")\n","# Save the cleaned DataFrame to a CSV file\n","df_cleaned.to_csv('cleaned_dataset.csv', index=False)\n","\n","print(\"Cleaned dataset saved as 'cleaned_dataset1.csv'\")\n"]},{"cell_type":"code","source":["'''This Python script is used to clean a dataset by removing specific patterns from text entries. It begins by loading a CSV file\n","('cleaned_dataset (2).csv') from a local directory into a Pandas DataFrame. A function is defined to identify and remove patterns\n","matching 'URL', 'EMAIL', or 'PHONE' followed by a 64-character hexadecimal string, which are typical for encoded or hashed data.\n","This function utilizes regular expressions for pattern matching. The script applies this cleaning function to every element of the\n","DataFrame, resulting in a cleaned version of the dataset. Finally, it saves the processed DataFrame to a new CSV file\n","('cleaned_dataset_no_specific_patterns.csv') in the same local directory. This ensures that sensitive or specific patterns are\n","removed from the dataset, making it more suitable for further analysis or sharing.'''"],"metadata":{"id":"9MsCDQ3c87YB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","\n","# Load the CSV file\n","file_path = 'C:/Users/prati/Downloads/cleaned_dataset (2).csv'\n","df = pd.read_csv(file_path)\n","\n","# Function to remove specific patterns (URL, EMAIL, PHONE) from a given text\n","def remove_specific_patterns(text):\n","    pattern = re.compile(r'(URL|EMAIL|PHONE)_[0-9a-fA-F]{64}')\n","    return pattern.sub(r'', str(text))\n","\n","# Apply the function to all columns in the DataFrame\n","df_cleaned = df.applymap(remove_specific_patterns)\n","\n","# Save the cleaned DataFrame to a new CSV file\n","output_file_path = 'C:/Users/prati/Downloads/cleaned_dataset_no_specific_patterns.csv'\n","df_cleaned.to_csv(output_file_path, index=False)\n","\n","output_file_path"],"metadata":{"id":"ZQVlEmF5czLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''This Python script is used to preprocess a dataset for text analysis by combining information from multiple columns into a unified text\n","representation. It begins by loading a CSV file ('final.csv') into a Pandas DataFrame. The script then merges text from several specified\n","columns ('title', 'location', 'description', and 'requirements') into a new column named 'combined_text'. This merging involves filling any\n","missing values with empty strings and concatenating the text from each row into a single string. Following this, the script extracts the\n","combined text and the corresponding labels into separate lists. The labels are sourced from a column named 'fraudulent', which is intended\n","to indicate whether each entry is fraudulent. This preprocessing step prepares the dataset for advanced text analysis techniques, such as\n","feature extraction with TF-IDF or deep learning models like BERT.'''"],"metadata":{"id":"7U2kUAzk9lgf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTU6oXPsB55d"},"outputs":[],"source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from transformers import BertTokenizer, BertModel\n","import torch\n","import numpy as np\n","\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Dataset/final.csv')\n","\n","# Combine text from relevant columns\n","text_columns = ['title', 'location', 'description', 'requirements']  # Adjust as needed\n","df['combined_text'] = df[text_columns].fillna('').astype(str).apply(lambda row: ' '.join(row), axis=1)\n","\n","# Extract the combined text and labels\n","text_data = df['combined_text'].tolist()\n","labels = df['fraudulent'].tolist()  # Update to use the correct label column"]},{"cell_type":"code","source":["'''This code snippet utilizes the 'TfidfVectorizer' from the 'sklearn.feature_extraction.text' module to transform the combined text\n","data into a numerical format suitable for machine learning models. It initializes the 'TfidfVectorizer' with a limit of 2000 features,\n","which means it will consider the top 2000 most important terms based on their term frequency-inverse document frequency (TF-IDF) scores.\n","The 'fit_transform' method is then applied to the 'text_data', which contains the combined text from the dataset. This method computes\n","the TF-IDF scores for each term in the text data and creates a sparse matrix ('tfidf_matrix') where each row represents a document\n","(or text entry) and each column represents a term. The matrix contains TF-IDF values that quantify the importance of each term in\n","each document, preparing the text data for further analysis or machine learning tasks.'''"],"metadata":{"id":"YchWemFa-Y1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYN2tqfnCHpi"},"outputs":[],"source":["tfidf_vectorizer = TfidfVectorizer(max_features=2000)\n","tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n"]},{"cell_type":"code","source":["'''This code snippet combines features extracted from TF-IDF and BERT to create a comprehensive feature set for text data. First, the\n","TF-IDF matrix, which was previously computed, is converted from a sparse matrix to a dense array using 'toarray()'. This dense\n","representation, 'tfidf_dense', captures the term frequency-inverse document frequency values for each document. Simultaneously,\n","BERT embeddings, which provide contextualized representations of the text, are assumed to be available in 'bert_embeddings'.\n","The script then horizontally stacks ('np.hstack()') these two feature sets—TF-IDF features and BERT embeddings—into a single array\n","called 'combined_features'. This array integrates the information from both feature extraction methods, enhancing the dataset with\n","a richer and more informative set of features that can be used for further analysis or machine learning models.'''"],"metadata":{"id":"ZtyuBimV-sP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXnsDn6XCBJY"},"outputs":[],"source":["# Combine TF-IDF and BERT features\n","tfidf_dense = tfidf_matrix.toarray()\n","combined_features = np.hstack((tfidf_dense, bert_embeddings))"]},{"cell_type":"code","source":["'''This code snippet utilizes BERT (Bidirectional Encoder Representations from Transformers) to generate contextual embeddings for a set\n","of text data. It begins by loading the pre-trained BERT tokenizer and model ('bert-base-uncased'). The script processes the text data in\n","batches of 16 to manage memory usage and efficiency. For each batch, it tokenizes the text into a format suitable for BERT using the tokenizer,\n","ensuring that text sequences are truncated and padded to a maximum length of 256 tokens. The tokenized inputs are then passed through the BERT\n","model to obtain embeddings. Specifically, the embeddings are extracted from the model’s last hidden state, focusing on the '[CLS]' token (the\n","first token of each sequence) to represent the entire sequence. These embeddings are converted from PyTorch tensors to NumPy arrays and appended\n","to a list. After processing all batches, the list of embeddings is concatenated into a single NumPy array ('bert_embeddings'), creating a dense\n","representation of the text data that captures rich contextual information.'''"],"metadata":{"id":"tb-4L1Zf_gqu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","batch_size = 16\n","bert_embeddings = []\n","for i in range(0, len(text_data), batch_size):\n","    batch_text = text_data[i:i+batch_size]\n","    inputs = tokenizer(batch_text, return_tensors='pt', truncation=True, padding=True, max_length=256)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n","    bert_embeddings.append(batch_embeddings)\n","\n","bert_embeddings = np.vstack(bert_embeddings)"],"metadata":{"id":"DPuVDDgpi0ai"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''This code snippet trains and evaluates a 'RandomForestClassifier' for text classification. It begins by splitting the dataset into\n","training and testing sets, reserving 20% for testing. The classifier is then trained on the training data and used to predict the test\n","data. The model’s performance is assessed by calculating accuracy and generating a detailed classification report, which includes precision,\n","recall, and F1-score metrics for each class. The results are printed to provide an overview of how effectively the model classifies the text data.'''"],"metadata":{"id":"vjKdDYcH_62E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72091,"status":"ok","timestamp":1724950309354,"user":{"displayName":"Vaishnavi Raman Shinde","userId":"03069921352989706694"},"user_tz":-330},"id":"UkzlCX2zCD6z","outputId":"55e5b859-2937-4088-e4c0-b26bc35f801d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9647137150466045\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      1.00      0.98      1419\n","           1       1.00      0.36      0.53        83\n","\n","    accuracy                           0.96      1502\n","   macro avg       0.98      0.68      0.76      1502\n","weighted avg       0.97      0.96      0.96      1502\n","\n"]}],"source":["# Train and evaluate the model\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","X_train, X_test, y_train, y_test = train_test_split(combined_features, labels, test_size=0.2, random_state=42)\n","\n","classifier = RandomForestClassifier()\n","classifier.fit(X_train, y_train)\n","\n","predictions = classifier.predict(X_test)\n","accuracy = accuracy_score(y_test, predictions)\n","report = classification_report(y_test, predictions)\n","\n","print(f'Accuracy: {accuracy}')\n","print('Classification Report:')\n","print(report)"]}],"metadata":{"colab":{"provenance":[{"file_id":"11095ifgUNwxoWocDgT7IuEmBUTDTvuEt","timestamp":1725033624454}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}